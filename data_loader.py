#!/usr/bin/python3
#
# ------------------------------------------------------------------------------
# Author: Gaowei Xu (gaowexu1991@gmail.com)
# ------------------------------------------------------------------------------
import os
import cv2
import json
import torch
import random
import imageio
import numpy as np
from enum import Enum
from functools import partial
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader


class Phase(Enum):
    Training = 1
    Validation = 2
    Test = 3


class NeRFDataset(Dataset):
    def __init__(
        self,
        phase: Phase = Phase.Training,
        root_data_dir: str = "./data/lego",
        near: float = 2.0,
        far: float = 6.0,
        enable_half_resolution: bool = True,
    ) -> None:
        super().__init__()
        self._phase = phase
        self._root_data_dir = root_data_dir
        self._near = near
        self._far = far
        self._enable_half_resolution = enable_half_resolution

        (
            self._images,  # (N, H, W, C), C = 4 which indicates RGBA
            self._poses,  # (N, 4, 4)
            self._image_height,
            self._image_width,
            self._focal,
            self._intrinsics,  # (3, 3)
        ) = self.collect_samples()

        print("Loading samples for phase {} = {}".format(phase, len(self._images)))

        self._rays = list()
        for pose in self._poses:
            # For each camera pose, we could calculate all the rays generated by this pose:
            #
            # rays_o.shape = (H, W, 3)
            # rays_d.shape = (H, W, 3)
            rays_o, rays_d = self.get_rays(
                self._image_height, self._image_width, self._intrinsics, pose
            )

            rays = torch.stack([rays_o, rays_d], dim=0)  # (2, H, W, 3)
            self._rays.append(rays)

        # self._rays.shape = (N, 2, H, W, 3), the last dimension 3 indicates (x, y, z), i.e.,
        # origin or direction vector.
        #
        # self._rays[:, 0, :, :, :] indicates rays' origins.
        # self._rays[:, 1, :, :, :] indicates rays' direction vector.
        self._rays = torch.stack(self._rays, dim=0)

        # self._rays.shape = (N, 3, H, W, 3), 3 indicates (x, y, z) or (r, g, b).
        # self._rays[:, 0, :, :, :] indicates rays' origins.
        # self._rays[:, 1, :, :, :] indicates rays' direction vector.
        # self._rays[:, 2, :, :, :] indicates image data.
        self._rays = torch.concat([self._rays, self._images[:, None, :, :, 0:3]], dim=1)

        # self._rays.shape: (N, 3, H, W, 3) --> (N, H, W, 3, 3).
        #
        # The last dimension is (x, y, z) or (r, g, b).
        # The second last dimension is {0, 1, 2}, 0 indicates rays' origin, 1 indicates rays'
        # direction vector, 2 indicates image RGB data.
        self._rays = torch.permute(self._rays, (0, 2, 3, 1, 4))

        # self._rays.shape: (N, H, W, 3, 3) --> (N*H*W, 3, 3).
        self._rays = torch.reshape(self._rays, shape=(-1, 3, 3))
        self._rays = self._rays.float()

        print("Generating rays with shape {}".format(self._rays.shape))
        return

    def collect_samples(self):
        if self._phase == Phase.Training:
            json_path = os.path.join(self._root_data_dir, "transforms_train.json")
        elif self._phase == Phase.Validation:
            json_path = os.path.join(self._root_data_dir, "transforms_val.json")
        elif self._phase == Phase.Test:
            json_path = os.path.join(self._root_data_dir, "transforms_test.json")
        else:
            raise RuntimeError("Phase {} not supported.".format(self._phase))

        data = json.load(open(json_path, "r"))

        images = list()
        poses = list()
        for frame in data["frames"]:
            image_path = os.path.join(self._root_data_dir, frame["file_path"] + ".png")
            image = imageio.v3.imread(image_path)
            images.append(image)
            poses.append(np.array(frame["transform_matrix"]))

        # For lego dataset: images.shape = (N, H, W, C), C = 4 which indicates RGBA, poses.shape = (N, 4, 4).
        #
        # The data shape for each phase is:
        # - Training:    N = 100
        # - Validation:  N = 100
        # - Test:        N = 200
        images = (np.array(images) / 255.0).astype(np.float32)
        poses = np.array(poses).astype(np.float32)

        image_raw_height, image_raw_width, image_raw_channels = images[0].shape

        # In lego dataset, camera_angle_x (unit: rad) is the horizontal field of view.
        # The following relationship holds for the camera:
        #
        # focal_length = 0.50 * image_width / np.tan(0.50 * camera_angle_x)
        horizontal_fov = float(data["camera_angle_x"])
        focal_length = 0.50 * image_raw_width / np.tan(0.50 * horizontal_fov)

        # If enable half-resolution, the images should be resized to original's 1/4 and the focal length
        # also should be updated to half of orginal value.
        if self._enable_half_resolution:
            resized_image_height = image_raw_height // 2
            resized_image_width = image_raw_width // 2
            focal_length = focal_length / 2.0

            resized_images = np.zeros(
                shape=(
                    images.shape[0],
                    resized_image_height,
                    resized_image_width,
                    image_raw_channels,
                )
            )

            for i, img in enumerate(images):
                resized_images[i] = cv2.resize(
                    img,
                    (resized_image_width, resized_image_height),
                    interpolation=cv2.INTER_AREA,
                )

            intrinsics = np.array(
                [
                    [focal_length, 0.0, 0.5 * resized_image_width],
                    [0.0, focal_length, 0.5 * resized_image_height],
                    [0.0, 0.0, 1.0],
                ]
            )

            return (
                torch.from_numpy(
                    resized_images
                ),  # (N, H, W, C), C = 4 which indicates RGBA
                torch.from_numpy(poses),  # (N, 4, 4)
                resized_image_height,
                resized_image_width,
                focal_length,
                torch.from_numpy(intrinsics),
            )

        else:
            intrinsics = np.array(
                [
                    [focal_length, 0.0, 0.5 * image_raw_width],
                    [0.0, focal_length, 0.5 * image_raw_height],
                    [0.0, 0.0, 1.0],
                ]
            )

            return (
                torch.from_numpy(images),  # (N, H, W, C), C = 4 which indicates RGBA
                torch.from_numpy(poses),  # (N, 4, 4)
                image_raw_height,
                image_raw_width,
                focal_length,
                torch.from_numpy(intrinsics),
            )

    @staticmethod
    def pose_spherical(theta: float, phi: float, radius: float):
        trans_t = lambda t: torch.Tensor(
            [
                [1.0, 0.0, 0.0, 0.0],
                [0.0, 1.0, 0.0, 0.0],
                [0.0, 0.0, 1.0, t],
                [0.0, 0.0, 0.0, 1.0],
            ]
        ).float()

        rot_phi = lambda phi: torch.Tensor(
            [
                [1.0, 0.0, 0.0, 0.0],
                [0.0, np.cos(phi), -np.sin(phi), 0.0],
                [0.0, np.sin(phi), np.cos(phi), 0.0],
                [0.0, 0.0, 0.0, 1.0],
            ]
        ).float()

        rot_theta = lambda th: torch.Tensor(
            [
                [np.cos(th), 0.0, -np.sin(th), 0.0],
                [0.0, 1.0, 0.0, 0.0],
                [np.sin(th), 0, np.cos(th), 0.0],
                [0.0, 0.0, 0.0, 1.0],
            ]
        ).float()

        cam2world_extrinsic = trans_t(radius)
        cam2world_extrinsic = rot_phi(phi / 180.0 * np.pi) @ cam2world_extrinsic
        cam2world_extrinsic = rot_theta(theta / 180.0 * np.pi) @ cam2world_extrinsic
        cam2world_extrinsic = (
            torch.Tensor(
                np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])
            )
            @ cam2world_extrinsic
        )
        return cam2world_extrinsic

    def get_rays(
        self,
        image_height: int,
        image_width: int,
        instrincis: torch.Tensor,
        cam2world_extrinsics: torch.Tensor,
    ):
        i, j = torch.meshgrid(
            torch.linspace(0, image_width - 1, image_width),
            torch.linspace(0, image_height - 1, image_height),
            indexing="ij",
        )

        i = i.t()  # i.shape = (H, W) after transpose operation
        j = j.t()  # j.shape = (H, W) after transpose operation

        # ray directions: dirs.shape = (H, W, 3)
        dirs = torch.stack(
            [
                (i - instrincis[0][2]) / instrincis[0][0],
                -(j - instrincis[1][2]) / instrincis[1][1],
                -torch.ones_like(i),
            ],
            dim=-1,
        )

        # Rotate ray directions from camera frame to the world frame.
        dirs = torch.unsqueeze(dirs, dim=-2)  # (H, W, 3) -> (H, W, 1, 3)
        rays_d = torch.sum(dirs * cam2world_extrinsics[:3, :3], -1)

        # Translate camera frame's origin to the world frame.
        rays_o = cam2world_extrinsics[:3, -1].expand(rays_d.shape)

        # rays_o.shape = (H, W, 3)
        # rays_d.shape = (H, W, 3)
        return rays_o, rays_d

    @staticmethod
    def get_render_poses():
        render_poses = torch.stack(
            [
                NeRFDataset.pose_spherical(angle, -30.0, 4.0)
                for angle in np.linspace(-180, 180, 40 + 1)[:-1]
            ],
            0,
        )

        return render_poses  # shape = (40, 4, 4)

    # def visualize_camera_poses(self):
    #     dirs = np.stack(
    #         [np.sum([0, 0, -1] * pose[:3, :3], axis=-1) for pose in self._poses]
    #     )  # 只要第三列, 并把方向逆一下
    #     origins = self._poses[:, :3, -1]  # 选前三行, 最后一列

    #     ax = plt.figure(figsize=(12, 8)).add_subplot(projection="3d")
    #     _ = ax.quiver(
    #         origins[..., 0].flatten(),
    #         origins[..., 1].flatten(),
    #         origins[..., 2].flatten(),
    #         dirs[..., 0].flatten(),
    #         dirs[..., 1].flatten(),
    #         dirs[..., 2].flatten(),
    #         length=0.5,
    #         normalize=True,
    #     )

    #     ax.set_xlabel("X")
    #     ax.set_ylabel("Y")
    #     ax.set_zlabel("z")

    #     plt.show()

    # def visualize_camera_rays(self):
    #     ax = plt.figure(figsize=(12, 8)).add_subplot(projection="3d")
    #     pose_id = 90
    #     _ = ax.quiver(
    #         ray_origin.cpu()[::20, ::20, 0],
    #         ray_origin.cpu()[::20, ::20, 1],
    #         ray_origin.cpu()[::20, ::20, 2],
    #         ray_direction.cpu()[::20, ::20, 0],
    #         ray_direction.cpu()[::20, ::20, 1],
    #         ray_direction.cpu()[::20, ::20, 2],
    #         length=3,
    #         normalize=True,
    #         arrow_length_ratio=0.05,
    #     )
    #     ax.set_xlabel("X")
    #     ax.set_ylabel("Y")
    #     ax.set_zlabel("Z")
    #     ax.set_xlim3d([-2, 2])
    #     ax.set_ylim3d([-2, 2])
    #     ax.set_zlim3d([0, 2])

    #     plt.show()

    def __len__(self):
        return self._rays.shape[0]

    def __getitem__(self, index):
        # self._rays.shape: (N*H*W, 3, 3).
        #
        # sample.shape = (3, 3)
        sample = self._rays[index]

        rays_with_direction = sample[0:2]  # (2, 3)
        target_rgb = sample[2]  # (3, )

        return rays_with_direction, target_rgb

    def collate_batch(self, batch_list, _unused=True):
        batch_rays_with_direction = list()
        batch_target_rgb = list()

        for index, (rays_with_direction, target_rgb) in enumerate(batch_list):
            batch_rays_with_direction.append(rays_with_direction)
            batch_target_rgb.append(target_rgb)

        batch_rays_with_direction = torch.stack(batch_rays_with_direction, dim=0)
        batch_target_rgb = torch.stack(batch_target_rgb, dim=0)

        return (
            batch_rays_with_direction,  # torch.Tensor with shape (B, 2, 3)
            batch_target_rgb,  # torch.Tensor with shape (B, 3)
        )


def worker_init_fn(worker_id, seed=666):
    if seed is not None:
        random.seed(seed + worker_id)
        np.random.seed(seed + worker_id)
        torch.manual_seed(seed + worker_id)
        torch.cuda.manual_seed(seed + worker_id)
        torch.cuda.manual_seed_all(seed + worker_id)


def compile_data(batch_size: int = 4096, num_workers: int = 8):
    train_data = NeRFDataset(phase=Phase.Training)
    val_data = NeRFDataset(phase=Phase.Validation)
    test_data = NeRFDataset(phase=Phase.Test)

    train_loader = DataLoader(
        dataset=train_data,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        drop_last=True,
        worker_init_fn=partial(worker_init_fn, seed=None),
        collate_fn=train_data.collate_batch,
        sampler=None,
    )

    val_loader = DataLoader(
        dataset=val_data,
        batch_size=batch_size,
        shuffle=False,
        drop_last=True,
        worker_init_fn=partial(worker_init_fn, seed=None),
        num_workers=num_workers,
        collate_fn=val_data.collate_batch,
    )

    test_loader = DataLoader(
        dataset=test_data,
        batch_size=batch_size,
        shuffle=False,
        drop_last=True,
        worker_init_fn=partial(worker_init_fn, seed=None),
        num_workers=num_workers,
        collate_fn=val_data.collate_batch,
    )

    return train_loader, val_loader, test_loader


if __name__ == "__main__":
    train_loader, val_loader, test_loader = compile_data()

    for batch_index, (batch_rays_with_direction, batch_target_rgb) in enumerate(
        train_loader
    ):
        print(
            "Training Batch {}/{}: batch_rays_with_direction.shape = {}, batch_target_rgb.shape = {}".format(
                batch_index,
                len(train_loader),
                batch_rays_with_direction.shape,
                batch_target_rgb.shape,
            )
        )

    for batch_index, (batch_rays_with_direction, batch_target_rgb) in enumerate(
        val_loader
    ):
        print(
            "Validation Batch {}/{}: batch_rays_with_direction.shape = {}, batch_target_rgb.shape = {}".format(
                batch_index,
                len(val_loader),
                batch_rays_with_direction.shape,
                batch_target_rgb.shape,
            )
        )

    for batch_index, (batch_rays_with_direction, batch_target_rgb) in enumerate(
        test_loader
    ):
        print(
            "Test Batch {}/{}: batch_rays_with_direction.shape = {}, batch_target_rgb.shape = {}".format(
                batch_index,
                len(test_loader),
                batch_rays_with_direction.shape,
                batch_target_rgb.shape,
            )
        )
